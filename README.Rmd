---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# desirability2

<!-- badges: start -->
[![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://lifecycle.r-lib.org/articles/stages.html#experimental)
[![Codecov test coverage](https://codecov.io/gh/tidymodels/desirability2/graph/badge.svg)](https://app.codecov.io/gh/tidymodels/desirability2)
[![R-CMD-check](https://github.com/tidymodels/desirability2/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/tidymodels/desirability2/actions/workflows/R-CMD-check.yaml)
<!-- badges: end -->

Desirability functions are simple but useful tools for simultaneously optimizing several things at once. For each input, a translation function is used to map the input values between zero and one where zero is unacceptable and one is most desirable. 

For example, [Kuhn and Johnson (2019)](https://bookdown.org/max/FES/genetic-algorithms.html#coercing-sparsity) use these functions during feature selection to help a genetic algorithm choose which predictors to include in a model that simultaneously improves performance and reduces the number of predictors. 

The desirability2 package improves on the original [desirability package](https://cran.r-project.org/package=desirability) by enabling in-line computations that can be used with dplyr pipelines. 

## A ranking example

Suppose a classification model with two tuning parameters (`penalty` and `mixture`) and several performance measures (multinomial log-loss, area under the precision-recall curve, and the area under the ROC curve). For each tuning parameter, the average number of features used in the model was also computed: 

```{r, start, include = FALSE}
library(desirability2)
library(dplyr)
library(tune)
library(ggplot2)
```

```{r}
library(desirability2)
library(dplyr)
classification_results
```

We might want to pick a model in a way that maximizes the area under the ROC curve with a minimum number of model terms. We know that the ROC measures is usually between 0.5 and 1.0. We can define a desirability function to _maximize_ this value using:

```r
d_max(roc_auc, low = 1/2, high = 1)
```

For the number of terms, if we wanted to minimize this under the condition that there should be less than 100 features, a minimal desirability function can be appropriate: 

```r
d_min(num_features, low = 1, high = 100)
```

We can add these as columns to the data using a `mutate()` statement along with a call to the function that blends these values using a geometric mean: 

```{r}
classification_results |> 
  select(-mn_log_loss, -pr_auc) |> 
  mutate(
    d_roc   = d_max(roc_auc, low = 1/2, high = 1), 
    d_terms = d_min(num_features, low = 1, high = 50),
    d_both    = d_overall(d_roc, d_terms)
  ) |> 
  # rank from most desirable to least:
  arrange(desc(d_both))
```

See `?inline_desirability` for details on the individual desirability functions. 

## Using with the tune package

We've also developed analogs to [`select_best()`](https://tune.tidymodels.org/reference/select_best.html) and [`show_best()`](https://tune.tidymodels.org/reference/show_best.html) that can be used to jointly optimize multiple metrics directly from an object produced by the tune and finetune packages. We can load the results for `tune::tune_grid()` for a pre-made binary file: 

```{r}
#| label: imbalance-example
library(tune)
library(ggplot2)

# Load the results from a cost-sensitive neural network:
load(system.file("example-data", "imbalance_example.rda", package = "desirability2"))

tuning_params <- .get_tune_parameter_names(imbalance_example)
tuning_params

metrics <- .get_tune_metric_names(imbalance_example)
metrics
```

To demonstrate, we've fitted a neural network with several tuning parameters, including one for `class_weights`. This parameter makes the model _cost-sensitive_; if a weight of 2.0 is used, the minority class in the training set has a 2-fold greater impact on the estimation process.

For this example data set, increasing the class weight decreases specificity and increases sensitivity (at different rates). However, if we optimize on sensitivity, we are likely to get the worst specificity value. 

```{r}
#| label: choices
nn_mtr <- collect_metrics(imbalance_example, type = "wide")
nn_mtr |> 
  ggplot(aes(1 - specificity, sensitivity, col = class_weights)) + 
  geom_point() + 
  coord_obs_pred() +
  theme_bw() +
  theme(legend.position = "top")

select_best(imbalance_example, metric = "sensitivity")

select_best(imbalance_example, metric = "specificity")

select_best(imbalance_example, metric = "brier_class")
```

Also, we want to make sure our model is well calibrated so we also want to minimize the Brier score.

To do this, we can use different verbs: `maximize()`, `minimize()`, `target()`, or `constrain()` for any of the metrics _or_ tuning parameters. For example: 

```{r}
#| label: multi-select
top_five <- 
  imbalance_example |> 
  show_best_desirability(
    # Put a little extra importance on the Brier scpre
    minimize(brier_class, scale = 2),
    maximize(sensitivity),
    constrain(specificity, low = 0.75, high = 1)
  )

# The resulting desirability functions and their overall mean
top_five |> 
  select(starts_with(".d_"))

# The metric values (for most desirable to the fifth least desirable):
top_five |> 
  select(all_of(metrics))
```

## Code of Conduct
  
Please note that the desirability2 project is released with a [Contributor Code of Conduct](https://contributor-covenant.org/version/2/0/CODE_OF_CONDUCT.html). By contributing to this project, you agree to abide by its terms.

## Contributing

This project is released with a [Contributor Code of Conduct](https://contributor-covenant.org/version/2/0/CODE_OF_CONDUCT.html). By contributing to this project, you agree to abide by its terms.

- If you think you have encountered a bug, please [submit an issue](https://github.com/tidymodels/desirability2/issues).

- Either way, learn how to create and share a [reprex](https://reprex.tidyverse.org/articles/articles/learn-reprex.html) (a minimal, reproducible example), to clearly communicate about your code.
