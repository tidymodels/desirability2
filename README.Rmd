---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# desirability2

<!-- badges: start -->
[![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://lifecycle.r-lib.org/articles/stages.html#experimental)
[![Codecov test coverage](https://codecov.io/gh/tidymodels/desirability2/graph/badge.svg)](https://app.codecov.io/gh/tidymodels/desirability2)
[![R-CMD-check](https://github.com/tidymodels/desirability2/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/tidymodels/desirability2/actions/workflows/R-CMD-check.yaml)
<!-- badges: end -->

Desirability functions are simple but useful tools for simultaneously optimizing several things at once. For each input, a translation function is used to map the input values between zero and one where zero is unacceptable and one is most desirable. 

For example, [Kuhn and Johnson (2019)](https://bookdown.org/max/FES/genetic-algorithms.html#coercing-sparsity) use these functions during feature selection to help a genetic algorithm choose which predictors to include in a model that simultaneously improves performance and reduces the number of predictors. 

The desirability2 package improves on the original [desirability package](https://cran.r-project.org/package=desirability) by enabling in-line computations that can be used with dplyr pipelines. 

## A ranking example

Suppose a classification model with two tuning parameters (`penalty` and `mixture`) and several performance measures (multinomial log-loss, area under the precision-recall curve, and the area under the ROC curve). For each tuning parameter, the average number of features used in the model was also computed: 

```{r, start, include = FALSE}
library(desirability2)
library(dplyr)
```

```{r}
library(desirability2)
library(dplyr)
classification_results
```

We might want to pick a model in a way that maximizes the area under the ROC curve with a minimum number of model terms. We know that the ROC measures is usually between 0.5 and 1.0. We can define a desirability function to _maximize_ this value using:

```r
d_max(roc_auc, low = 1/2, high = 1)
```

For the number of terms, if we wanted to minimize this under the condition that there should be less than 100 features, a minimal desirability function can be appropriate: 

```r
d_min(num_features, low = 1, high = 100)
```

We can add these as columns to the data using a `mutate()` statement along with a call to the function that blends these values using a geometric mean: 

```{r}
classification_results |> 
  select(-mn_log_loss, -pr_auc) |> 
  mutate(
    d_roc   = d_max(roc_auc, low = 1/2, high = 1), 
    d_terms = d_min(num_features, low = 1, high = 50),
    d_both    = d_overall(d_roc, d_terms)
  ) |> 
  # rank from most desirable to least:
  arrange(desc(d_both))
```

See `?inline_desirability` for details on the individual desirability functions. 


## Code of Conduct
  
Please note that the desirability2 project is released with a [Contributor Code of Conduct](https://contributor-covenant.org/version/2/0/CODE_OF_CONDUCT.html). By contributing to this project, you agree to abide by its terms.

## Contributing

This project is released with a [Contributor Code of Conduct](https://contributor-covenant.org/version/2/0/CODE_OF_CONDUCT.html). By contributing to this project, you agree to abide by its terms.

- If you think you have encountered a bug, please [submit an issue](https://github.com/tidymodels/desirability2/issues).

- Either way, learn how to create and share a [reprex](https://reprex.tidyverse.org/articles/articles/learn-reprex.html) (a minimal, reproducible example), to clearly communicate about your code.
